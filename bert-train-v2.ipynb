{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "194dcdbe-6ad9-4b02-95a2-2db255f85590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "035eb393a0d443b7ad8b7f98b403390c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e16aa55b37ee489db042f2b1305f4875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset \n",
    "dataset = load_dataset(\"allenai/c4\", \"en\", streaming=True)\n",
    "#print(dataset)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca5c19ec-e16e-46b6-856e-69f5ce6a2d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: dict_keys(['text', 'timestamp', 'url'])\n",
      "{'text': 'Beginners BBQ Class Taking Place in Missoula!\\nDo you want to get better at making delicious BBQ? You will have the opportunity, put this on your calendar now. Thursday, September 22nd join World Class BBQ Champion, Tony Balay from Lonestar Smoke Rangers. He will be teaching a beginner level class for everyone who wants to get better with their culinary skills.\\nHe will teach you everything you need to know to compete in a KCBS BBQ competition, including techniques, recipes, timelines, meat selection and trimming, plus smoker and fire information.\\nThe cost to be in the class is $35 per person, and for spectators it is free. Included in the cost will be either a t-shirt or apron and you will be tasting samples of each meat that is prepared.', 'timestamp': '2019-04-25 12:57:54', 'url': 'https://klyq.com/beginners-bbq-class-taking-place-in-missoula/'}\n"
     ]
    }
   ],
   "source": [
    "first_example = next(iter(dataset[\"train\"]))\n",
    "print(f\"Keys: {first_example.keys()}\")\n",
    "print(first_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db2114a2-3ee5-4eee-b0e7-17a19d93cb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split those data into files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd958f2f-9382-459f-984c-ebcce0db7d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "ALREADY_PROCESSED = True\n",
    "\n",
    "if not ALREADY_PROCESSED:\n",
    "    text_data = []\n",
    "    file_count = 0\n",
    "    \n",
    "    for sample in tqdm(dataset['train']):\n",
    "        sample = sample['text'].replace('\\n', '')\n",
    "        sample = sample.lower()\n",
    "        text_data.append(sample)\n",
    "        if len(text_data) == 6_000:\n",
    "            # once we git the 6K mark, save to file\n",
    "            with open(f'c4_files/text_{file_count}.txt', 'w', encoding='utf-8') as fp:\n",
    "                fp.write('\\n'.join(text_data))\n",
    "            text_data = []\n",
    "            file_count += 1\n",
    "        if file_count >= 100:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c5f1e9da-2d0b-4d6f-b518-f78bdfd21e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "import torch\n",
    "from transformers import BertConfig, BertForMaskedLM\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "# 加载预训练模型的配置（不加载权重）\n",
    "config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "#print(config)\n",
    "\n",
    "# fp32 by default\n",
    "model = BertForMaskedLM(config)\n",
    "model.to(device);\n",
    "\n",
    "# If continue from previous epoch\n",
    "#model.load_state_dict(torch.load('bert-trained/bert_ckpt_5.pth'))\n",
    "\n",
    "#for name, param in model.named_parameters():\n",
    "#    print(f\"{name:50} shape: {str(param.shape):20} dtype: {param.dtype} param: {param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "175bcf2e-a6c3-4d64-b840-c49a19adfe50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "# from transformers import BertTokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad2db3a6-bb39-49d4-b3bd-ced76554c474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分词结果: ['hello', ',', 'i', 'love', 'natural', 'language', 'processing', '!']\n",
      "Token IDs: [7592, 1010, 1045, 2293, 3019, 2653, 6364, 999]\n",
      "编码结果: [101, 7592, 1010, 1045, 2293, 3019, 2653, 6364, 999, 102]\n",
      "解码: [CLS] hello, i love natural language processing! [SEP]\n"
     ]
    }
   ],
   "source": [
    "# test tokenizer\n",
    "text = \"Hello, I love natural language processing!\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"分词结果:\", tokens)\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"Token IDs:\", input_ids)\n",
    "\n",
    "encoded = tokenizer.encode(text)\n",
    "print(\"编码结果:\", encoded)\n",
    "print(\"解码:\", tokenizer.decode(encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1de4bf49-61d3-4e81-819e-2e36fc88d2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the input pipeline and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa1a17d-99e2-4478-bb60-9aec1721b5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "INTERVAL=5\n",
    "TARGET=25\n",
    "\n",
    "#with open('c4_files/text_0.txt', 'r', encoding='utf-8') as fp:\n",
    "#    lines = fp.read().split('\\n') \n",
    "\n",
    "lines = []\n",
    "for id in range(TARGET-INTERVAL, TARGET):\n",
    "    file_path = f\"c4_files/text_{id}.txt\"\n",
    "    print(file_path)\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines.extend(f.read().strip().split('\\n'))\n",
    "#print(lines[0])\n",
    "\n",
    "#lines = [\n",
    "#    \"In the morning, I went to the school.\",\n",
    "#    \"In the afternoon, I went to the school.\",\n",
    "#    \"In the evening, I went to the school.\",\n",
    "#    \"I love to eat bread for breakfast.\",\n",
    "#    \"I love to eat butter for breakfast.\",\n",
    "#    \"I love to eat noodle for breakfast.\",\n",
    "#    \"The weather today is very good.\",\n",
    "#    \"The weather today is very calm.\",\n",
    "#    \"The weather today is very stormy.\",\n",
    "#    \"he is a good man.\",\n",
    "#    \"he is a gentle man.\",\n",
    "#    \"he is a polite man.\",\n",
    "#    \"She is a doctor.\",\n",
    "#    \"She is a nurse.\",\n",
    "#    \"She is a teacher.\",\n",
    "#]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "830d49d3-3ac6-4d40-b7fd-257c08ffdd4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ae17088a88b456ba23167f76e6dafce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62dcc8ed5347452684d4f9ac55ad1045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d5d0fafd3754d3285490d5340fc230c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0843ed45cd094971beb087eb12de5da0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b3ceb6bf9db4ddda6335c80dad8ab04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7e80d2475ad486fa50668b5009f42ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be034bed822b4fe2b42b3a6c93d071ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da6dae42186e404f8b4acb4a87a178bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df8816b920e2433f821eae757413abb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4adac1391b234ec4bd8cf424db3109c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "tokenized_lines = tokenizer(lines, max_length=512, padding='max_length', truncation=True, return_tensors='pt')\n",
    "#list(tokenized_lines.keys())\n",
    "\n",
    "## Mask raw inputs\n",
    "import torch\n",
    "\n",
    "input_ids = tokenized_lines['input_ids']\n",
    "attention_mask = tokenized_lines['attention_mask']\n",
    "\n",
    "rand = torch.rand(input_ids.shape)\n",
    "#print(rand)\n",
    "# mask random 15% where token is not special tokens, id >= 999\n",
    "mask_arr = (rand < 0.15) * (input_ids >= 999)\n",
    "#print(mask_arr)\n",
    "masked_input_ids = torch.where(mask_arr, tokenizer.mask_token_id, input_ids)\n",
    "#print(masked_input_ids)\n",
    "\n",
    "labels = input_ids.clone()\n",
    "labels[attention_mask == 0] = -100\n",
    "#print(labels)\n",
    "\n",
    "\n",
    "# Define data loader\n",
    "encodings = {'input_ids': masked_input_ids, 'attention_mask': attention_mask, 'labels': labels} \n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        # store encodings internally\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __len__(self):\n",
    "        # return the number of samples\n",
    "        return self.encodings['input_ids'].shape[0]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # return dictionary of input_ids, attention_mask, and labels for index i\n",
    "        return {key: tensor[i] for key, tensor in self.encodings.items()}\n",
    "\n",
    "dataset = Dataset(encodings) \n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True) \n",
    "\n",
    "\n",
    "# Training\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# activate training mode\n",
    "model.train()\n",
    "# initialize optimizer\n",
    "optim = AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Train with autocast bf16\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # setup loop with TQDM and dataloader\n",
    "    loop = tqdm(loader, leave=True)\n",
    "    for batch in loop:\n",
    "        # initialize calculated gradients (from prev step)\n",
    "        optim.zero_grad()\n",
    "        # pull all tensor batches required for training\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        # process\n",
    "        # mixed precision automatically\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "            outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                            labels=labels)\n",
    "        # extract loss\n",
    "        loss = outputs.loss\n",
    "        # calculate loss for every parameter that needs grad update\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optim.step()\n",
    "        # print relevant info to progress bar\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "781f8424-02a7-4dbb-8e06-98b9beecf6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "print(TARGET)\n",
    "torch.save(model.state_dict(), f\"bert-trained/bert_ckpt_{TARGET}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "777b1b3d-0ec2-4b8b-a1e7-51b21f46ab25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "输入: In the [MASK], I went to the school.\n",
      "  1. to              (score: 0.2340)\n",
      "  2. today           (score: 0.2187)\n",
      "  3. afternoon       (score: 0.0073)\n",
      "  4. a               (score: 0.0063)\n",
      "  5. evening         (score: 0.0060)\n",
      "\n",
      "输入: I love to eat [MASK] for breakfast.\n",
      "  1. .               (score: 0.3847)\n",
      "  2. bread           (score: 0.1910)\n",
      "  3. no              (score: 0.0174)\n",
      "  4. ##odle          (score: 0.0040)\n",
      "  5. very            (score: 0.0023)\n",
      "\n",
      "输入: The weather today is very [MASK].\n",
      "  1. .               (score: 0.1605)\n",
      "  2. the             (score: 0.0993)\n",
      "  3. calm            (score: 0.0846)\n",
      "  4. ##odle          (score: 0.0343)\n",
      "  5. good            (score: 0.0075)\n",
      "\n",
      "输入: [MASK] is a good man.\n",
      "  1. she             (score: 0.9631)\n",
      "  2. he              (score: 0.0017)\n",
      "  3. the             (score: 0.0008)\n",
      "  4. .               (score: 0.0003)\n",
      "  5. to              (score: 0.0002)\n",
      "\n",
      "输入: She is a [MASK] doctor.\n",
      "  1. nurse           (score: 0.9528)\n",
      "  2. .               (score: 0.0003)\n",
      "  3. gentle          (score: 0.0001)\n",
      "  4. she             (score: 0.0001)\n",
      "  5. teacher         (score: 0.0001)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate with fill mask \n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# 创建fill-mask pipeline\n",
    "fill_mask = pipeline(\n",
    "    'fill-mask',\n",
    "    model=model,  # 你的训练好的模型\n",
    "    tokenizer=tokenizer,  # 对应的tokenizer\n",
    "    device=0 if torch.cuda.is_available() else -1,  # GPU/CPU\n",
    "    top_k=5  # 显示前5个预测结果\n",
    ")\n",
    "\n",
    "# 测试句子（包含[MASK] token）\n",
    "test_sentences = [\n",
    "    \"In the [MASK], I went to the school.\",\n",
    "    \"I love to eat [MASK] for breakfast.\",\n",
    "    \"The weather today is very [MASK].\",\n",
    "    \"[MASK] is a good man.\",\n",
    "    \"She is a [MASK] doctor.\"\n",
    "]\n",
    "\n",
    "# 逐个预测\n",
    "for sentence in test_sentences:\n",
    "    print(f\"\\n输入: {sentence}\")\n",
    "    results = fill_mask(sentence)\n",
    "    for i, result in enumerate(results):\n",
    "        print(f\"  {i+1}. {result['token_str']:15} (score: {result['score']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3194ea-e6eb-4c8c-88c7-e49c6cda3102",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
